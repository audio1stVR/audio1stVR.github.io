<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>NIME Workshop on Audio-first VR</title>
  <link rel="stylesheet" type="text/css" href="style.css">

</head>

<body>
  <div id="title">NIME Workshop on Audio-first VR</div>
  <div id="bio-container">
    <div class="bio">
      <p>We are rapidly approaching the mass adoption of virtual reality as a
      consumer-oriented entertainment medium. Recent breakthroughs in
      low-persistence displays combined with modern head-tracking techniques
      have enabled the design of head-mounted systems that offer plausible
      virtual experiences. The role of sound in establishing a convincing sense
      of immersion in such experiences has been acknowledged since the early
      days of VR. Morton Heilig’s 1960 patent for a stereoscopic television
      apparatus, which is one of the earliest concept designs for a VR system,
      describes the use of ear phones and binaural sound. Ivan Sutherland, the
      designer of what is considered to be the first head-mounted display, wrote
      in 1965 that despite the availability of excellent audio output devices,
      the computers had not yet been capable of producing meaningful sounds that
      can be integrated into “the ultimate display.” Since then, significant
      advances in digital computing has facilitated the development of real-time
      audio synthesis and spatialization techniques. Modern consumer-grade
      computers are capable of executing complex sound-field synthesis
      algorithms that are suitable for VR applications.</p>

      <p>Leading technology companies today are growingly invested in spatial audio
      with new products, such as Facebook’s Spatial Workstation and Google’s
      Resonance Audio, that facilitate the use of Ambisonics and binaural audio
      with popular VR platforms. However, most modern VR systems impart an
      ancillary role to audio, where sounds serve to amplify visual immersion.
      It is up to sound and music researchers to elaborate ways in which we can
      think natively about VR audio. This workshop aims to investigate the
      concept of an audio-first VR as a medium for musical expression, and
      identify multimodal experiences that focus on the auditory aspects of
      immersion rather than those that are ocular. Through a day-long workshop
      that involves presentations and demo sessions followed by round table
      discussions, the participants will collectively address such questions as:
      What is a VR experience that originates from sonic phenomena? How do we
      behave and interact within such experiences? What are the tools that we
      have or need to create these experiences? What are the implications of an
      audio-centric VR for musical expression? As a result, the participants
      will outline a position on what constitutes an audio-first VR from the
      perspective of the NIME community.</p>

      <p>We seek abstract submissions that focus on the following topics among others:
        <li>Sonic virtual realities
        <li>Immersive sonification
        <li>Virtual interfaces for musical expression (VIMEs)
        <li>Creativity support tools for VR audio
        <li>Visualizing an audio-first VR
        <li>Spatial audio techniques for VR
        <li>Embodied interactions within virtual audio systems
        <li>Applications of computer music theory to VR
        <li>Composing music for VR games and films
        <li>Sonic VR as assistive technology
        <li>Histories of sound in virtual environments

      </p>Abstracts of approximately 350 words should be sent as a PDF file to
      acamci@umich.edu with "NIME Workshop on Audio-first VR" included in the
      subject line. The accepted abstracts and the proceedings of the workshop will
      be published on the workshop website at audio1stVR.github.io. Number of
      presenting participants will be limited to 10. However, attendance will be
      open to non-presenting participants within space limits.

      <div class="title">Workshop Structure</div>
        <li>9:00-12:00 – 15-minute presentations by the participants
        <li>12:00-13:00 – Lunch break
        <li>13:00-14:00 – Demo session (a room-scale VR system and a 4-channel audio system will be provided)
        <li>14:00-17:00 – Discussion, identifying a preliminary framework for audio-first VRs

      <!-- <div class="title">Workshop Leaders</div>
      <b>Anıl Çamcı</b> is an Assistant Professor of Performing Arts Technology at the
      University of Michigan. His work investigates new tools and theories for
      multimodal worldmaking using a variety of media ranging from electronic music to
      virtual reality. Çamcı’s research has been
      featured in leading journals, such as Organised Sound, Soundscape and IEEE CG&A.
      He presented his work at top-tier conferences including ACM UIST, ACM CHI, IEEE
      VR, ICMC and NIME. He has been granted several awards and scholarships,
      including the Audio Engineering Society Fellowship, and the ACM CHI Artist
      Grant. -->

    </div>
  </div>
</body>

</html>
